# 交差検証法


```python
from sklearn.model_selection import KFold, cross_val_score

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

tree_score = cross_val_score(tree_model, X, y, cv=kfold)

print(tree_score)
print(tree_score.mean())
```

## 例

```python
from sklearn import tree
# 分類器モデルの構築
model = tree.DecisionTreeClassifier(random_state=42)
# 学習の実施
tree_model = model.fit(X_train,y_train)

# 交差検証
from sklearn.model_selection import KFold, cross_val_score

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

tree_score = cross_val_score(tree_model, X, y, cv=kfold)

print(tree_score)
print(tree_score.mean())
```

```
[1.         0.96666667 0.93333333 0.9        0.93333333]
0.9466666666666667
```

## 例

```python
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV

# チューニングするパラメータ
tuned_parameters = {
    "learning_rate":[0.05, 0.1, 0.2],
    "n_estimators":[1, 25, 50, 100],     # 作成するデータセットの数＝モデルの数
    "max_depth":[3, 5, 7, 10],
    "min_child_weight":[3, 5, 7, 10],
}

clf = GridSearchCV(
    XGBClassifier(random_state=42),
    tuned_parameters,
    scoring="accuracy",
    cv=5,
    n_jobs=1
)

clf = clf.fit(X_train, y_train) # モデル作成
best_clf = clf.best_estimator_  # 最も精度がよいモデルを取得

print("Best Parameter : ", clf.best_params_)

# テストデータの予測値を確率で出力する
pred_best = best_clf.predict_proba(X_test)[:, 1]
# テストデータの予測値を二値に変換する
pred_best_label = np.where(pred_best > 0.5, 1, 0)

from sklearn.metrics import log_loss, accuracy_score
from sklearn.model_selection import KFold

# 各foldのスコアを保存するリスト
score_accuracy = []
score_logloss = []

# クロスバリデーションの実施
kf = KFold(n_splits=5, shuffle=True, random_state=42)
for tr_idx, va_idx in kf.split(X_train):
    # 訓練データを訓練データとバリデーションデータに分ける
    tr_x, va_x = X_train.iloc[tr_idx], X_train.iloc[va_idx]
    tr_y, va_y = y_train.iloc[tr_idx], y_train.iloc[va_idx]
    
    # 学習の実施
    best_clf.fit(tr_x, tr_y)
    
    # バリデーションの予測値を確率で出力する
    va_pred = best_clf.predict_proba(va_x)[:, 1]
    
    # バリデーションデータのスコアを計算する
    logloss = log_loss(va_y, va_pred)
    accuracy = accuracy_score(va_y, va_pred > 0.5)
    
    # foldのスコアを保存する
    score_logloss.append(logloss)
    score_accuracy.append(accuracy)

# 各foldのスコアの平均を出力する
logloss = np.mean(score_logloss)
accuracy = np.mean(score_accuracy)
print(f'logloss: {logloss:.4f}, accuracy: {accuracy:.4f}')
```

## scikit-learnのKFoldを使用して、任意のデータをk等分に分割することができます。

k-foldを使ったクロスバリデーションでは、各サンプルに0からk-1までの値が割り当てられます。


```python
from sklearn import model_selection

# kfold という名前の新しいカラムを作成し、それを -1 で埋めます。
df['kfold'] = -1

# データの行をランダム化する
df = df.sample(frac=1).reset_index(drop=True)

# kfoldクラスを起動
kf = model_selection.KFold(n_splits=5)

# kfold列を埋める
for fold, (trn_, val_) in enumerate(kf.split(X=df)):
    df.loc[val_, 'kfold'] = fold
    
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>kfold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.2</td>
      <td>0.635</td>
      <td>0.07</td>
      <td>2.6</td>
      <td>0.077</td>
      <td>16.0</td>
      <td>86.0</td>
      <td>0.99748</td>
      <td>3.51</td>
      <td>0.54</td>
      <td>9.7</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.9</td>
      <td>0.550</td>
      <td>0.15</td>
      <td>2.2</td>
      <td>0.076</td>
      <td>19.0</td>
      <td>40.0</td>
      <td>0.99610</td>
      <td>3.41</td>
      <td>0.59</td>
      <td>10.1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.2</td>
      <td>0.340</td>
      <td>0.38</td>
      <td>2.5</td>
      <td>0.080</td>
      <td>12.0</td>
      <td>57.0</td>
      <td>0.99780</td>
      <td>3.30</td>
      <td>0.47</td>
      <td>9.0</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10.2</td>
      <td>0.645</td>
      <td>0.36</td>
      <td>1.8</td>
      <td>0.053</td>
      <td>5.0</td>
      <td>14.0</td>
      <td>0.99820</td>
      <td>3.17</td>
      <td>0.42</td>
      <td>10.0</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.1</td>
      <td>0.680</td>
      <td>0.07</td>
      <td>1.9</td>
      <td>0.075</td>
      <td>16.0</td>
      <td>51.0</td>
      <td>0.99685</td>
      <td>3.38</td>
      <td>0.52</td>
      <td>9.5</td>
      <td>2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
df['kfold'].value_counts()
```




    3    320
    2    320
    1    320
    0    320
    4    319
    Name: kfold, dtype: int64




```python
df.to_csv("train_folds.csv", index=False) 
```

## 層別k-fold<br>

```python
from sklearn import model_selection

df = pd.read_csv('./data/winequality-red.csv', sep=';')
# kfold という名前の新しいカラムを作成し、それを -1 で埋めます。
df['kfold'] = -1

# データの行をランダム化する
df = df.sample(frac=1).reset_index(drop=True)

# qualityを取り出す
y = df['quality'].values

# kfoldクラスを起動
kf = model_selection.StratifiedKFold(n_splits=5)

# kfold列を埋める
for fold, (trn_, val_) in enumerate(kf.split(X=df, y=y)):
    df.loc[val_, 'kfold'] = fold
    
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>kfold</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6.9</td>
      <td>0.49</td>
      <td>0.10</td>
      <td>2.3</td>
      <td>0.074</td>
      <td>12.0</td>
      <td>30.0</td>
      <td>0.99590</td>
      <td>3.42</td>
      <td>0.58</td>
      <td>10.2</td>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5.3</td>
      <td>0.57</td>
      <td>0.01</td>
      <td>1.7</td>
      <td>0.054</td>
      <td>5.0</td>
      <td>27.0</td>
      <td>0.99340</td>
      <td>3.57</td>
      <td>0.84</td>
      <td>12.5</td>
      <td>7</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2</td>
      <td>0.58</td>
      <td>0.03</td>
      <td>2.3</td>
      <td>0.077</td>
      <td>7.0</td>
      <td>28.0</td>
      <td>0.99568</td>
      <td>3.35</td>
      <td>0.52</td>
      <td>10.0</td>
      <td>5</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.3</td>
      <td>0.39</td>
      <td>0.31</td>
      <td>2.4</td>
      <td>0.074</td>
      <td>9.0</td>
      <td>46.0</td>
      <td>0.99620</td>
      <td>3.41</td>
      <td>0.54</td>
      <td>9.4</td>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8.4</td>
      <td>0.39</td>
      <td>0.10</td>
      <td>1.7</td>
      <td>0.075</td>
      <td>6.0</td>
      <td>25.0</td>
      <td>0.99581</td>
      <td>3.09</td>
      <td>0.43</td>
      <td>9.7</td>
      <td>6</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
df['kfold'].value_counts()
```




    3    320
    2    320
    1    320
    0    320
    4    319
    Name: kfold, dtype: int64


