# 単回帰

```python

```

### 書式
	model = LinearRegression()
	# 教師データ(学習を行うための既存データ)を用いて学習器に学習させます。
	model.fit(train_X, train_y)

	# 教師データとは別に用意したテスト用データを用いて学習器に予測させます。
	pred_y = model.predict(test_X)

	# 学習器の性能を確認するため決定係数という評価値を算出します。
	score = model.score(test_X, test_y)

### 例

```python
# 必要なモジュールのインポート
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# データの生成
X, y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=5.0, random_state=42)
train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)

# 以下にコードを記述してください。
# モデルの構築
model = LinearRegression()
# モデルの学習
model.fit(train_X, train_y)
# test_X, test_yに対する決定係数を出力してください
print(model.score(test_X, test_y))
```

### 説明
<b style='color: #AA0000'>線形単回帰</b>とは、1つの予測したいデータ(ex. 水の量)を<b>1つのデータ(ex. 時間)から求める回帰分析</b>です。データの関係性を調べるときに使うことが多く、予測を行うときに用いられることは稀です。

ここでは予測したいデータを$y$、予測に用いるデータを$x$として、 

$y=ax+b$

という関係があると仮定して、$a$と$b$を推定します。このチャプターの「線形回帰とは」の水の量のたとえと同じ形の数式なのがわかります(a=4、b=0)。

$a$と$b$の推定には様々な方法がありますが、今回は<b style='color: #AA0000'>最小二乗法</b>と呼ばれる方法を用いましょう。
実際の$y$の値と推定する$y(=ax+b)$の値の差の<b>二乗の総和</b>が最小になるように$a$と$b$を定める方法です。下図で言うと、オレンジのデータ点からの距離の総和が最小になるようにa、bを決めます。
このようにしてすでにあるデータに対して一番近い直線を引き、その直線から今後のデータを推測します。

なお、ここで誤差を二乗するのは何故でしょうか？それは<b>二乗することによって、正負の相違による誤差の相殺</b>がされないようにするためです。例えば誤差が+2と-2のものを単純に足し合わせると、値が0になって誤差が相殺されてしまいます。

さて、実際に回帰分析を行うには、scikit-learnのlinear_modelモジュール内にあるLinearRegressionというモデルを使うのが便利でしょう。