# 重回帰

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# モデルの構築
model = LinearRegression()
# モデルの学習
model.fit(train_X, train_y)
# test_X, test_yに対する決定係数を出力してください
print(model.score(test_X, test_y))
```

### 例

```python
# 必要なモジュールのインポート
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# データの生成
X, y = make_regression(n_samples=100, n_features=1, n_targets=1, noise=5.0, random_state=42)
train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)

# モデルの構築
model = LinearRegression()
# モデルの学習
model.fit(train_X, train_y)
# test_X, test_yに対する決定係数を出力してください
print(model.score(test_X, test_y))
```

### 説明
<b style='color: #AA0000'>線形重回帰</b>とは、予測したいデータが1つ(ex2. レストランの総合評価の点数)に対し、<b>予測に用いるデータが複数個</b>(ex2. 食べ物のおいしさの点数と接客の良さの点数)となる回帰分析です。
予測に用いられるデータ同士の関係性が薄いときには高い予測精度が得られます。

ここでも最小二乗法をもちいて予測するデータと予測に用いるデータの関係性を推定します。  
重回帰の場合は予測に用いるデータを$x_0$,$x_1$,$x_2$...として

$y = \beta_{0}x_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \cdots +\epsilon$  

となるような$\beta_{0},\beta_{1}, \beta_{2}..., \epsilon$を推定することになります。

前のエクササイズで扱った数式に比べると、どうでしょう？$x$の種類が増えてそれに応じて係数（先ほどのセッション「線形単回帰」でいう$a$）をたくさん設定しなくてはならなくなった、というイメージがありますね。

線形重回帰もscikit-learnの`linear_model`モジュール内にある`LinearRegression`というモデルを使って回帰分析を行うことが可能です。  自動的に、すでにあるデータに対して一番フィットするように$\beta_0,\beta_1, \beta_2..., \epsilon$が決定され、予測が行われます。

```Python
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

# ここでn_features=10とすることでxを生成します
# 実際に使用するxの数はn_informative=3といった様に指定します
X, y = make_regression(n_samples=100, n_features=10, n_informative=3, n_targets=1, noise=5.0, random_state=42)
train_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)

model = LinearRegression()
model.fit(train_X, train_y)
model.score(test_X, test_y)
```
また、`model.predict(test_X)`と書くことで`test_X`に対する予測を行うことができます。